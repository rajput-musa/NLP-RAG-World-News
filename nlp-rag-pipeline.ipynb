{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T15:23:28.032520Z",
     "iopub.status.busy": "2025-06-22T15:23:28.032270Z",
     "iopub.status.idle": "2025-06-22T15:23:47.781522Z",
     "shell.execute_reply": "2025-06-22T15:23:47.780759Z",
     "shell.execute_reply.started": "2025-06-22T15:23:28.032502Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "featuretools 1.31.0 requires tqdm>=4.66.3, but you have tqdm 4.66.1 which is incompatible.\n",
      "preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "textblob 0.19.0 requires nltk>=3.9, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mLibraries installed. Please restart runtime if you installed new critical packages like bitsandbytes or ollama for the first time.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q \\\n",
    "    pandas pyarrow datasets==2.19.* \\\n",
    "    sentence-transformers==2.7.0 \\\n",
    "    faiss-cpu==1.8.0 \\\n",
    "    rank_bm25==0.2.2 \\\n",
    "    nltk==3.8.1 \\\n",
    "    tqdm==4.66.1 \\\n",
    "    transformers==4.40.* \\\n",
    "    accelerate==0.29.* \\\n",
    "    bitsandbytes==0.43.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T15:23:49.867351Z",
     "iopub.status.busy": "2025-06-22T15:23:49.867081Z",
     "iopub.status.idle": "2025-06-22T15:23:59.057903Z",
     "shell.execute_reply": "2025-06-22T15:23:59.057152Z",
     "shell.execute_reply.started": "2025-06-22T15:23:49.867329Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import gc\n",
    "import time\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import nltk\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer, util, CrossEncoder\n",
    "import faiss\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# --- Basic Configuration ---\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" \n",
    "nltk.download('punkt', quiet=True)\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# For reproducibility with some operations\n",
    "if torch.cuda.is_available() and hasattr(torch.backends.cudnn, 'deterministic'):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T15:27:19.384710Z",
     "iopub.status.busy": "2025-06-22T15:27:19.384359Z",
     "iopub.status.idle": "2025-06-22T15:27:19.389187Z",
     "shell.execute_reply": "2025-06-22T15:27:19.388573Z",
     "shell.execute_reply.started": "2025-06-22T15:27:19.384684Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"/kaggle/input/all-the-news-dataset\") \n",
    "MUST_ENTS  = [\"Ossetia\", \"Abkhazia\", \"Brexit\", \"COVID-19\", \"Ukraine War\", \"Gaza conflict\"] \n",
    "TARGET_SZ  = 30_000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T15:27:43.509465Z",
     "iopub.status.busy": "2025-06-22T15:27:43.509151Z",
     "iopub.status.idle": "2025-06-22T15:42:16.397013Z",
     "shell.execute_reply": "2025-06-22T15:42:16.396378Z",
     "shell.execute_reply.started": "2025-06-22T15:27:43.509440Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "379e4467b7df4d4695d636728069be55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scanning CSVs: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚑ forced-entities found: 79165 rows\n",
      "Other documents found: 2505000 rows\n",
      "Sampled 'must_df' down to 30000 rows as it met or exceeded target size.\n",
      "✅ Final subset shape: (30000, 6)\n",
      "                                                   title  \\\n",
      "26980  Credit Suisse to cut jobs as it pares back in ...   \n",
      "77592  Iconic photos: The USNS Comfort arriving in Ne...   \n",
      "27824  New Manhattan condos break price records, for now   \n",
      "43987  Twitter Will Let You Temporarily Follow Olympi...   \n",
      "66340  Ford to shut Spanish factory for one week due ...   \n",
      "\n",
      "                                                 article  \\\n",
      "26980  ZURICH (Reuters) - Credit Suisse (CSGN.S) is t...   \n",
      "77592  Two days after President Donald Trump took par...   \n",
      "27824  The numbers are soaring higher than the buildi...   \n",
      "43987  Twitter is about to embark on a 17 day-long te...   \n",
      "66340  MADRID (Reuters) - Ford (F.N) said on Sunday i...   \n",
      "\n",
      "                                                     url    publication  \\\n",
      "26980  https://www.reuters.com/article/us-britain-eu-...        Reuters   \n",
      "77592  https://www.cnbc.com/2020/03/30/iconic-photos-...           CNBC   \n",
      "27824  https://www.cnbc.com/2016/07/01/new-manhattan-...           CNBC   \n",
      "43987  https://www.buzzfeednews.com/article/alexkantr...  Buzzfeed News   \n",
      "66340  https://www.reuters.com/article/health-coronav...        Reuters   \n",
      "\n",
      "       doc_id                                      text_to_embed  \n",
      "26980       0  Credit Suisse to cut jobs as it pares back in ...  \n",
      "77592       1  Iconic photos: The USNS Comfort arriving in Ne...  \n",
      "27824       2  New Manhattan condos break price records, for ...  \n",
      "43987       3  Twitter Will Let You Temporarily Follow Olympi...  \n",
      "66340       4  Ford to shut Spanish factory for one week due ...  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "use_cols   = {\"title\", \"article\", \"publication\", \"url\"} # Start with the most critical and clearly named ones\n",
    "\n",
    "must_rows, other_rows = [], []\n",
    "\n",
    "def csv_iter(data_dir, use_cols_set):\n",
    "    for fp in data_dir.glob(\"*.csv\"):\n",
    "        try:\n",
    "            \n",
    "            df_peek = pd.read_csv(fp, nrows=0)\n",
    "            current_file_cols = set(df_peek.columns)\n",
    "            cols_to_read = list(use_cols_set.intersection(current_file_cols))\n",
    "            if not cols_to_read: # Skip if no relevant columns\n",
    "                continue\n",
    "            \n",
    "            # Check for 'article' vs 'content'\n",
    "            if 'content' in cols_to_read and 'article' not in cols_to_read and 'article' in use_cols_set:\n",
    "                # If 'content' exists and 'article' is desired but not in file, map later\n",
    "                pass\n",
    "            elif 'article' in cols_to_read and 'content' not in cols_to_read and 'content' in use_cols_set:\n",
    "                 # If 'article' exists and 'content' is desired but not in file, map later\n",
    "                pass\n",
    "\n",
    "\n",
    "            yield from pd.read_csv(fp, usecols=cols_to_read,\n",
    "                               dtype=str, low_memory=False,\n",
    "                               chunksize=20_000) # Smaller chunksize for memory\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping file {fp} due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "for chunk_idx, chunk in enumerate(tqdm(csv_iter(DATA_DIR, use_cols), desc=\"Scanning CSVs\")):\n",
    "    # Standardize 'article' column name\n",
    "    if \"content\" in chunk.columns and \"article\" not in chunk.columns:\n",
    "        chunk = chunk.rename(columns={\"content\": \"article\"})\n",
    "    elif \"article\" in chunk.columns and \"content\" not in chunk.columns:\n",
    "        # We'll use 'article' as the standard internally\n",
    "        pass\n",
    "    elif \"content\" in chunk.columns and \"article\" in chunk.columns:\n",
    "        # If both exist, prioritize 'article' or define a rule\n",
    "    \n",
    "        chunk['article'] = chunk['article'].fillna(chunk['content'])\n",
    "        chunk = chunk.drop(columns=['content'], errors='ignore')\n",
    "\n",
    "\n",
    "    if \"article\" not in chunk.columns: # Skip if no article content\n",
    "        continue\n",
    "\n",
    "    chunk = chunk.dropna(subset=[\"article\"])\n",
    "    chunk['title'] = chunk['title'].fillna(\"\") # Fill NaN titles\n",
    "\n",
    "    mask  = chunk[\"article\"].str.contains(\"|\".join(MUST_ENTS), case=False, na=False)\n",
    "    must_rows.append(chunk[mask])\n",
    "    other_rows.append(chunk[~mask])\n",
    "    \n",
    "\n",
    "if not must_rows and not other_rows:\n",
    "    raise ValueError(\"No data loaded. Check DATA_DIR path and CSV files.\")\n",
    "\n",
    "must_df  = pd.concat(must_rows,  ignore_index=True) if must_rows else pd.DataFrame()\n",
    "other_df = pd.concat(other_rows, ignore_index=True) if other_rows else pd.DataFrame()\n",
    "\n",
    "print(f\"⚑ forced-entities found: {len(must_df)} rows\")\n",
    "print(f\"Other documents found: {len(other_df)} rows\")\n",
    "\n",
    "if len(must_df) >= TARGET_SZ:\n",
    "    df = must_df.sample(n=TARGET_SZ, random_state=RANDOM_SEED, replace=False)\n",
    "    print(f\"Sampled 'must_df' down to {TARGET_SZ} rows as it met or exceeded target size.\")\n",
    "else:\n",
    "    remaining_needed = TARGET_SZ - len(must_df)\n",
    "    if len(other_df) >= remaining_needed:\n",
    "        sample_other = other_df.sample(n=remaining_needed, random_state=RANDOM_SEED, replace=False)\n",
    "        df = pd.concat([must_df, sample_other], ignore_index=True)\n",
    "    else: # If other_df is too small to fill the remaining\n",
    "        df = pd.concat([must_df, other_df], ignore_index=True)\n",
    "        print(f\"Warning: Could only gather {len(df)} rows, less than target {TARGET_SZ}, as 'other_df' was too small.\")\n",
    "\n",
    "# Create a unique ID for each document\n",
    "df['doc_id'] = range(len(df))\n",
    "df[\"text_to_embed\"] = df[\"title\"].fillna(\"\") + \". \" + df[\"article\"].str.strip()\n",
    "\n",
    "print(f\"✅ Final subset shape: {df.shape}\")\n",
    "print(df.head())\n",
    "\n",
    "# Clean up large intermediate dataframes\n",
    "del must_rows, other_rows, must_df, other_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T16:22:27.641089Z",
     "iopub.status.busy": "2025-06-22T16:22:27.640614Z",
     "iopub.status.idle": "2025-06-22T16:22:49.965569Z",
     "shell.execute_reply": "2025-06-22T16:22:49.964758Z",
     "shell.execute_reply.started": "2025-06-22T16:22:27.641055Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68363a6c8f494071a2818a9baad57e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunking documents:   0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 30000\n",
      "Total chunks created: 278947\n",
      "   doc_id                                     original_title  \\\n",
      "0       0  Credit Suisse to cut jobs as it pares back in ...   \n",
      "1       0  Credit Suisse to cut jobs as it pares back in ...   \n",
      "2       0  Credit Suisse to cut jobs as it pares back in ...   \n",
      "3       0  Credit Suisse to cut jobs as it pares back in ...   \n",
      "4       0  Credit Suisse to cut jobs as it pares back in ...   \n",
      "\n",
      "                                        original_url  \\\n",
      "0  https://www.reuters.com/article/us-britain-eu-...   \n",
      "1  https://www.reuters.com/article/us-britain-eu-...   \n",
      "2  https://www.reuters.com/article/us-britain-eu-...   \n",
      "3  https://www.reuters.com/article/us-britain-eu-...   \n",
      "4  https://www.reuters.com/article/us-britain-eu-...   \n",
      "\n",
      "                                          chunk_text  chunk_id  \n",
      "0  Credit Suisse to cut jobs as it pares back in ...         0  \n",
      "1  talks with the European Union. UBS and Credit ...         1  \n",
      "2  Chief Executive Tidjane Thiam. One Credit Suis...         2  \n",
      "3  The bank does not provide a breakdown for the ...         3  \n",
      "4  bank Vontobel, said. “With Brexit, London has ...         4  \n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # Using LangChain's splitter\n",
    "\n",
    "# --- Chunking Parameters ---\n",
    "CHUNK_SIZE = 500  # Characters\n",
    "CHUNK_OVERLAP = 50 # Characters\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "all_chunks = []\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Chunking documents\"):\n",
    "    chunks = text_splitter.split_text(row['text_to_embed'])\n",
    "    for chunk_text in chunks:\n",
    "        all_chunks.append({\n",
    "            'doc_id': row['doc_id'], # Link back to original document\n",
    "            'original_title': row['title'],\n",
    "            'original_url': row.get('url', ''), # Get URL if exists\n",
    "            'chunk_text': chunk_text\n",
    "        })\n",
    "\n",
    "chunks_df = pd.DataFrame(all_chunks)\n",
    "chunks_df['chunk_id'] = range(len(chunks_df)) # Unique ID for each chunk\n",
    "\n",
    "print(f\"Total documents: {len(df)}\")\n",
    "print(f\"Total chunks created: {len(chunks_df)}\")\n",
    "print(chunks_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T16:22:53.835065Z",
     "iopub.status.busy": "2025-06-22T16:22:53.834316Z",
     "iopub.status.idle": "2025-06-22T16:23:05.706603Z",
     "shell.execute_reply": "2025-06-22T16:23:05.705924Z",
     "shell.execute_reply.started": "2025-06-22T16:22:53.835037Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870b662ffb004dbca924e746b684829b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing for BM25:   0%|          | 0/278947 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Indexed 278947 chunks using in-memory BM25\n",
      "⏱️ BM25 Indexing Time: 7.09 seconds\n"
     ]
    }
   ],
   "source": [
    "# BM25 expects a list of lists of tokens\n",
    "tokenized_chunks_for_bm25 = [doc.lower().split() for doc in tqdm(chunks_df['chunk_text'], desc=\"Tokenizing for BM25\")]\n",
    "\n",
    "start_time = time.time()\n",
    "bm25 = BM25Okapi(tokenized_chunks_for_bm25)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"✅ Indexed {len(tokenized_chunks_for_bm25)} chunks using in-memory BM25\")\n",
    "print(f\"⏱️ BM25 Indexing Time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T16:23:07.390062Z",
     "iopub.status.busy": "2025-06-22T16:23:07.389264Z",
     "iopub.status.idle": "2025-06-22T16:23:08.494358Z",
     "shell.execute_reply": "2025-06-22T16:23:08.493693Z",
     "shell.execute_reply.started": "2025-06-22T16:23:07.390035Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BM25 Results for: 'What happened in South Ossetia in 2008?'\n",
      "  Score: 30.0291 | Title: Russia and Georgia tensions are rising: Here's why... | Chunk: pro-Russian) self-proclaimed republics of South Ossetia and Abkhazia. As such the parliamentary addr...\n",
      "  Score: 29.6864 | Title: Front lines of a frozen conflict... | Chunk: By the numbers: There are roughly 240,000 people living in Abkhazia and 50,000 in South Ossetia. Ano...\n",
      "  Score: 29.4312 | Title: Hundreds of Georgians demand release of doctor det... | Chunk: The United States and rights group Amnesty International called for Gaprindashvili’s immediate relea...\n"
     ]
    }
   ],
   "source": [
    "def search_bm25(query: str, k: int = 5):\n",
    "    tokenized_query = query.lower().split()\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # Get top k indices and their scores\n",
    "    topk_indices_scores = sorted(zip(range(len(scores)), scores), key=lambda x: x[1], reverse=True)[:k]\n",
    "    \n",
    "    results = []\n",
    "    for i, score in topk_indices_scores:\n",
    "        chunk_info = chunks_df.iloc[i]\n",
    "        results.append({\n",
    "            'chunk_id': chunk_info['chunk_id'],\n",
    "            'doc_id': chunk_info['doc_id'],\n",
    "            'score': score,\n",
    "            'text': chunk_info['chunk_text'],\n",
    "            'title': chunk_info['original_title'],\n",
    "            'url': chunk_info['original_url']\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Test BM25\n",
    "test_query_bm25 = \"What happened in South Ossetia in 2008?\"\n",
    "bm25_results = search_bm25(test_query_bm25, k=3)\n",
    "print(f\"\\nBM25 Results for: '{test_query_bm25}'\")\n",
    "for res in bm25_results:\n",
    "    print(f\"  Score: {res['score']:.4f} | Title: {res['title'][:50]}... | Chunk: {res['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T16:23:31.757651Z",
     "iopub.status.busy": "2025-06-22T16:23:31.757108Z",
     "iopub.status.idle": "2025-06-22T16:23:40.400523Z",
     "shell.execute_reply": "2025-06-22T16:23:40.399894Z",
     "shell.execute_reply.started": "2025-06-22T16:23:31.757626Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d3c6b6be5140198dbc25f618c523b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb810dc7e1c4c66b5354ad07a550899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579be0f87b8240e4a7677a43292a9899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/11.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e3f579d3a243b3a074e1d031375b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8ad6e57f104b658fc981e13b6c9255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1083902691cd474781163264d1753a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64105ee05574420abaa9006a8152c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/383 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a10d7776034b471a865a1050f3fedecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4717d63eb5481bbbbc3491c3ba12f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad89456821bb4b478efdc6d31192ada5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c92e4a3d62c041109612929b8a3f1bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model 'multi-qa-MiniLM-L6-cos-v1' loaded. Max sequence length: 512\n"
     ]
    }
   ],
   "source": [
    "# --- Dense Retriever Parameters ---\n",
    "EMBEDDING_MODEL_NAME = 'multi-qa-MiniLM-L6-cos-v1' \n",
    "\n",
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=DEVICE)\n",
    "print(f\"Embedding model '{EMBEDDING_MODEL_NAME}' loaded. Max sequence length: {embedding_model.max_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T16:23:47.125324Z",
     "iopub.status.busy": "2025-06-22T16:23:47.124331Z",
     "iopub.status.idle": "2025-06-22T16:31:16.073658Z",
     "shell.execute_reply": "2025-06-22T16:31:16.072896Z",
     "shell.execute_reply.started": "2025-06-22T16:23:47.125296Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 278947 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4117c6addb410ca968886663dea287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings generated. Shape: torch.Size([278947, 384])\n",
      "⏱️ Embedding Time: 447.59 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_texts_to_embed = chunks_df['chunk_text'].tolist()\n",
    "\n",
    "print(f\"Generating embeddings for {len(chunk_texts_to_embed)} chunks...\")\n",
    "start_time = time.time()\n",
    "\n",
    "chunk_embeddings = embedding_model.encode(\n",
    "    chunk_texts_to_embed,\n",
    "    batch_size=128, \n",
    "    show_progress_bar=True,\n",
    "    convert_to_tensor=True,\n",
    "    device=DEVICE\n",
    ")\n",
    "end_time = time.time()\n",
    "print(f\"✅ Embeddings generated. Shape: {chunk_embeddings.shape}\")\n",
    "print(f\"⏱️ Embedding Time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Move embeddings to CPU for FAISS if they were on GPU\n",
    "chunk_embeddings_cpu = chunk_embeddings.cpu().numpy()\n",
    "if DEVICE == \"cuda\":\n",
    "    del chunk_embeddings # Free up GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T16:31:25.900840Z",
     "iopub.status.busy": "2025-06-22T16:31:25.900135Z",
     "iopub.status.idle": "2025-06-22T16:31:26.365488Z",
     "shell.execute_reply": "2025-06-22T16:31:26.364645Z",
     "shell.execute_reply.started": "2025-06-22T16:31:25.900814Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built. Total vectors in index: 278947\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = chunk_embeddings_cpu.shape[1]\n",
    "# Using IndexFlatL2 for simplicity, but IndexFlatIP (Inner Product) is often better for cosine similarity with normalized embeddings\n",
    "# If using cosine similarity, normalize embeddings before adding to FAISS and during query\n",
    "# For dot product (often used by SBERT models like multi-qa-...), normalization might not be strictly needed if model output is already scaled\n",
    "\n",
    "# Normalize embeddings for cosine similarity\n",
    "faiss.normalize_L2(chunk_embeddings_cpu)\n",
    "\n",
    "index_faiss = faiss.IndexFlatIP(embedding_dim) # IP (Inner Product) for normalized vectors is equivalent to cosine similarity\n",
    "index_faiss.add(chunk_embeddings_cpu)\n",
    "\n",
    "print(f\"FAISS index built. Total vectors in index: {index_faiss.ntotal}\")\n",
    "\n",
    "# Save the index (optional, but good for large datasets)\n",
    "# faiss.write_index(index_faiss, \"news_chunks.faiss_index\")\n",
    "# chunks_df.to_csv(\"news_chunks_metadata.csv\", index=False) # Save corresponding metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T16:31:34.237655Z",
     "iopub.status.busy": "2025-06-22T16:31:34.237366Z",
     "iopub.status.idle": "2025-06-22T16:31:34.358341Z",
     "shell.execute_reply": "2025-06-22T16:31:34.357503Z",
     "shell.execute_reply.started": "2025-06-22T16:31:34.237633Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a8e085eaa24d09b593f644e9f544b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FAISS Results for: 'What were the latest developments in the Brexit negotiations last year?'\n",
      "  Score: 0.7010 | Title: 10 things you should know about the Brexit breakth... | Chunk: 10 things you should know about the Brexit breakthrough. Brexit negotiators claimed Friday they had ...\n",
      "  Score: 0.6907 | Title: Sterling falls as traders prepare for EU summit ne... | Chunk: near their lowest since May 2017, according to the latest positioning data. Brexit negotiations have...\n",
      "  Score: 0.6836 | Title: Sterling volatility measures fall to 15-month lows... | Chunk: no significant Brexit-related developments were expected this week. (Reporting by Saikat Chatterjee ...\n"
     ]
    }
   ],
   "source": [
    "def search_faiss(query: str, k: int = 5):\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor=True, device=DEVICE)\n",
    "    query_embedding_cpu = query_embedding.cpu().numpy().reshape(1, -1) # Reshape for FAISS\n",
    "    faiss.normalize_L2(query_embedding_cpu) # Normalize query embedding\n",
    "\n",
    "    distances, indices = index_faiss.search(query_embedding_cpu, k)\n",
    "    \n",
    "    results = []\n",
    "    for i in range(len(indices[0])):\n",
    "        idx = indices[0][i]\n",
    "        score = distances[0][i]\n",
    "        chunk_info = chunks_df.iloc[idx]\n",
    "        results.append({\n",
    "            'chunk_id': chunk_info['chunk_id'],\n",
    "            'doc_id': chunk_info['doc_id'],\n",
    "            'score': score, # This is dot product; higher is better\n",
    "            'text': chunk_info['chunk_text'],\n",
    "            'title': chunk_info['original_title'],\n",
    "            'url': chunk_info['original_url']\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Test FAISS\n",
    "test_query_faiss = \"What were the latest developments in the Brexit negotiations last year?\"\n",
    "faiss_results = search_faiss(test_query_faiss, k=3)\n",
    "print(f\"\\nFAISS Results for: '{test_query_faiss}'\")\n",
    "for res in faiss_results:\n",
    "    print(f\"  Score: {res['score']:.4f} | Title: {res['title'][:50]}... | Chunk: {res['text'][:100]}...\")\n",
    "\n",
    "if DEVICE == \"cuda\" and 'query_embedding' in locals():\n",
    "    del query_embedding\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T16:31:55.095917Z",
     "iopub.status.busy": "2025-06-22T16:31:55.095307Z",
     "iopub.status.idle": "2025-06-22T16:31:59.571602Z",
     "shell.execute_reply": "2025-06-22T16:31:59.571050Z",
     "shell.execute_reply.started": "2025-06-22T16:31:55.095892Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d397a6e3fc844d919d76955fcbf20888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc5c9952ba004836afdc6cc0a772f078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b25cfa7e3514755a2ce85e01b5972dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69debd079c640d8abb0ed8eb750ea16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e918602ab514f4caed3b9740d883ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76fd5312f78540c7874718f53999795c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranker model 'cross-encoder/ms-marco-MiniLM-L-6-v2' loaded.\n"
     ]
    }
   ],
   "source": [
    "# --- Reranker Parameters ---\n",
    "CROSS_ENCODER_MODEL_NAME = 'cross-encoder/ms-marco-MiniLM-L-6-v2' # Small and fast\n",
    "# CROSS_ENCODER_MODEL_NAME = 'cross-encoder/ms-marco-TinyBERT-L-2-v2' # Even smaller\n",
    "# CROSS_ENCODER_MODEL_NAME = 'BAAI/bge-reranker-base' # Newer, often very good\n",
    "\n",
    "reranker_model = CrossEncoder(CROSS_ENCODER_MODEL_NAME, device=DEVICE, max_length=512) # max_length depends on model\n",
    "print(f\"Reranker model '{CROSS_ENCODER_MODEL_NAME}' loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T16:38:06.258796Z",
     "iopub.status.busy": "2025-06-22T16:38:06.258470Z",
     "iopub.status.idle": "2025-06-22T16:38:08.008926Z",
     "shell.execute_reply": "2025-06-22T16:38:08.008139Z",
     "shell.execute_reply.started": "2025-06-22T16:38:06.258773Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d350b0eff34d44db8491d12edab70a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample reranker_pair[0]: Query='Tell me about the impact of COVID-19 on global sup...', Text='the United States and China specifically. It's a c...'\n",
      "Reranking 20 candidate pairs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff668150494a4e888c494f5046d9b54c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Type of rerank_scores: <class 'numpy.ndarray'>\n",
      "DEBUG: Shape of rerank_scores: (20,)\n",
      "DEBUG: First 5 rerank_scores: [-3.0371718  6.472589  -1.5158455 -1.6148225 -1.8780392]\n",
      "\n",
      "Hybrid + Reranked Results for: 'Tell me about the impact of COVID-19 on global supply chains.'\n",
      "  Rerank Score: 6.4726 | BM25: 33.24 | FAISS: N/A\n",
      "  Title: Coronavirus: At least 150 companies have warned in... | Chunk: return to full production, and those delays will have a significant impact on freight volumes moved in Q1.\" It added that is also may further impact g...\n",
      "  URL: https://www.cnbc.com/2020/03/11/coronavirus-at-least-150-companies-have-warned-investors.html\n",
      "------------------------------\n",
      "  Rerank Score: 6.1507 | BM25: N/A | FAISS: N/A\n",
      "  Title: COVID-19 is attacking our defense supply chains an... | Chunk: COVID-19 is attacking our defense supply chains and our nation's security | TheHill. COVID-19 is leading to rolling shutdowns of commerce across the c...\n",
      "  URL: https://thehill.com/opinion/national-security/489375-covid-19-is-attacking-our-defense-supply-chains-and-our-nations\n",
      "------------------------------\n",
      "  Rerank Score: 5.8701 | BM25: N/A | FAISS: N/A\n",
      "  Title: Coronavirus wreaks havoc on retail supply chains g... | Chunk: providers to find paths forward to minimize disruption.\"  Similar to how a trade war accelerated the rate at which some retailers were trying to lesse...\n",
      "  URL: https://www.cnbc.com/2020/03/16/coronavirus-wreaks-havoc-on-retail-supply-chains-globally.html\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "def hybrid_search_and_rerank(query: str, bm25_k: int = 20, faiss_k: int = 20, rerank_top_n: int = 5):\n",
    "    # 1. Get results from BM25\n",
    "    bm25_res = search_bm25(query, k=bm25_k)\n",
    "    \n",
    "    # 2. Get results from FAISS\n",
    "    faiss_res = search_faiss(query, k=faiss_k)\n",
    "    \n",
    "    # 3. Combine and deduplicate, preserving original scores\n",
    "    combined_results_dict = {} # Use dict to deduplicate by chunk_id\n",
    "    \n",
    "    for res_item in bm25_res:\n",
    "        chunk_id = res_item['chunk_id']\n",
    "        if chunk_id not in combined_results_dict:\n",
    "            combined_results_dict[chunk_id] = res_item.copy()\n",
    "            combined_results_dict[chunk_id]['bm25_score'] = res_item['score']\n",
    "            if 'score' in combined_results_dict[chunk_id]: combined_results_dict[chunk_id].pop('score')\n",
    "        else:\n",
    "            combined_results_dict[chunk_id]['bm25_score'] = res_item['score']\n",
    "\n",
    "\n",
    "    for res_item in faiss_res:\n",
    "        chunk_id = res_item['chunk_id']\n",
    "        if chunk_id not in combined_results_dict:\n",
    "            combined_results_dict[chunk_id] = res_item.copy()\n",
    "            combined_results_dict[chunk_id]['faiss_score'] = res_item['score']\n",
    "            if 'score' in combined_results_dict[chunk_id]: combined_results_dict[chunk_id].pop('score')\n",
    "        else:\n",
    "            combined_results_dict[chunk_id]['faiss_score'] = res_item['score']\n",
    "            \n",
    "    candidate_chunks = list(combined_results_dict.values())\n",
    "    \n",
    "    if not candidate_chunks:\n",
    "        print(\"No candidate chunks to rerank.\") # Added print\n",
    "        return []\n",
    "        \n",
    "    # 4. Prepare pairs for reranker\n",
    "    reranker_pairs = []\n",
    "    for chunk_info in candidate_chunks:\n",
    "        reranker_pairs.append([query, chunk_info['text']])\n",
    "    \n",
    "    # DEBUG: Print a sample pair\n",
    "    if reranker_pairs:\n",
    "        print(f\"Sample reranker_pair[0]: Query='{reranker_pairs[0][0][:50]}...', Text='{reranker_pairs[0][1][:50]}...'\")\n",
    "            \n",
    "   # 5. Rerank\n",
    "    # Remove .device from this print statement:\n",
    "    print(f\"Reranking {len(reranker_pairs)} candidate pairs...\") # Corrected print\n",
    "    rerank_scores = [] # Initialize\n",
    "    if reranker_pairs:\n",
    "        try:\n",
    "            rerank_scores = reranker_model.predict(\n",
    "                reranker_pairs, \n",
    "                show_progress_bar=True, # Turn on progress bar for reranker\n",
    "                batch_size=32 # Try a smaller batch size for reranker initially\n",
    "            )\n",
    "            # ---- DEBUG PRINTS ----\n",
    "            print(f\"DEBUG: Type of rerank_scores: {type(rerank_scores)}\")\n",
    "            if hasattr(rerank_scores, 'shape'):\n",
    "                print(f\"DEBUG: Shape of rerank_scores: {rerank_scores.shape}\")\n",
    "            else:\n",
    "                print(f\"DEBUG: Length of rerank_scores (if list): {len(rerank_scores) if isinstance(rerank_scores, list) else 'Not a list/array with len'}\")\n",
    "            print(f\"DEBUG: First 5 rerank_scores: {rerank_scores[:5] if isinstance(rerank_scores, (list, np.ndarray)) and len(rerank_scores) > 0 else 'N/A or Empty'}\")\n",
    "            # ---- END DEBUG PRINTS ----\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR during reranker_model.predict: {e}\")\n",
    "            rerank_scores = [] # Ensure it's an empty list on error\n",
    "    else:\n",
    "        print(\"No pairs to rerank.\")\n",
    "\n",
    "    # 6. Add rerank scores and sort\n",
    "    # Ensure rerank_scores is a list/array of numbers\n",
    "    if not isinstance(rerank_scores, (list, np.ndarray)) or not all(isinstance(x, (int, float, np.number)) for x in rerank_scores):\n",
    "        print(f\"Warning: rerank_scores is not a list/array of numbers. Got: {rerank_scores}\")\n",
    "        # Fill with default low scores if it's not in the expected format\n",
    "        rerank_scores = [-float('inf')] * len(candidate_chunks)\n",
    "\n",
    "\n",
    "    for i, chunk_info in enumerate(candidate_chunks):\n",
    "        if i < len(rerank_scores):\n",
    "            chunk_info['rerank_score'] = float(rerank_scores[i]) # Ensure it's a float\n",
    "        else:\n",
    "            # This case should ideally not happen if len(rerank_scores) == len(candidate_chunks)\n",
    "            print(f\"Warning: Mismatch in lengths. i={i}, len(rerank_scores)={len(rerank_scores)}\")\n",
    "            chunk_info['rerank_score'] = -float('inf')\n",
    "\n",
    "    reranked_results = sorted(candidate_chunks, key=lambda x: x.get('rerank_score', -float('inf')), reverse=True)\n",
    "    \n",
    "    return reranked_results[:rerank_top_n]\n",
    "\n",
    "# Test Hybrid Search + Rerank (in the same cell or a new one)\n",
    "test_query_hybrid = \"Tell me about the impact of COVID-19 on global supply chains.\"\n",
    "hybrid_reranked_results = hybrid_search_and_rerank(test_query_hybrid, bm25_k=10, faiss_k=10, rerank_top_n=3)\n",
    "\n",
    "print(f\"\\nHybrid + Reranked Results for: '{test_query_hybrid}'\")\n",
    "for res in hybrid_reranked_results:\n",
    "    # Prepare score strings carefully\n",
    "    rerank_score_str = f\"{res['rerank_score']:.4f}\" if isinstance(res.get('rerank_score'), (int, float)) else \"N/A\"\n",
    "    bm25_score_val = res.get('bm25_score')\n",
    "    bm25_score_str = f\"{bm25_score_val:.2f}\" if isinstance(bm25_score_val, (int, float)) else \"N/A\"\n",
    "    faiss_score_val = res.get('faiss_score')\n",
    "    faiss_score_str = f\"{faiss_score_val:.2f}\" if isinstance(faiss_score_val, (int, float)) else \"N/A\"\n",
    "\n",
    "    print(f\"  Rerank Score: {rerank_score_str} | BM25: {bm25_score_str} | FAISS: {faiss_score_str}\")\n",
    "    print(f\"  Title: {res['title'][:50]}... | Chunk: {res['text'][:150]}...\")\n",
    "    print(f\"  URL: {res.get('url', 'N/A')}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T16:49:09.931113Z",
     "iopub.status.busy": "2025-06-22T16:49:09.930769Z",
     "iopub.status.idle": "2025-06-22T16:50:20.201167Z",
     "shell.execute_reply": "2025-06-22T16:50:20.200587Z",
     "shell.execute_reply.started": "2025-06-22T16:49:09.931088Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM: microsoft/Phi-3-mini-4k-instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74bcc55907044d91abce96336127e125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c9dd90480f41a6bdc23646a6f195b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6febe82daf944c795138a182c4e9907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89bda12926ce416e89962d09835d2b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d062a7b1af934f159c9d473e9e8b7327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba0f485342044c6a0959dd61cac1a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e89bd50f794be2a8826cc2278ecfd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3.py:   0%|          | 0.00/11.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc04db39e61a4741a64109283126e806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi3.py:   0%|          | 0.00/73.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cabf31a03f4346b4ac3f3811e24d6ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c4c7af90a7b437e983b3cf228eb2a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0097e75c89b464d99e7d78ccb4e805a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf2df77b5b94a3caf96c2bdd16ce668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42cec4df336848f69b0e8b2f7cc033c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7006202e387d413d905ecea5d63c3538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- LLM Parameters ---\n",
    "# We are switching to a smaller, very capable model that doesn't require complex quantization.\n",
    "LLM_MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# We don't need the BitsAndBytesConfig anymore, which was causing the error.\n",
    "\n",
    "try:\n",
    "    print(f\"Loading LLM: {LLM_MODEL_NAME}...\")\n",
    "    llm_tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME, trust_remote_code=True)\n",
    "    \n",
    "    # Load the model directly to the device. 'torch_dtype=\"auto\"' helps with performance.\n",
    "    llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "        LLM_MODEL_NAME,\n",
    "        torch_dtype=\"auto\", # Automatically uses bfloat16 on compatible GPUs\n",
    "        device_map=\"auto\",  # Automatically maps to GPU if available\n",
    "        trust_remote_code=True # Required for Phi-3 model architecture\n",
    "    )\n",
    "    llm_model.eval() # Set to evaluation mode\n",
    "    print(\"LLM loaded successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading LLM: {e}\")\n",
    "    print(\"This might be a VRAM issue. If so, try restarting the kernel and running again.\")\n",
    "    llm_model = None\n",
    "    llm_tokenizer = None\n",
    "\n",
    "# Set pad token if not set, common for generative models\n",
    "if llm_tokenizer and llm_tokenizer.pad_token is None:\n",
    "    # Llama-3 and Phi-3 use the EOS token as the PAD token.\n",
    "    llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "    if hasattr(llm_model, 'config'):\n",
    "         llm_model.config.pad_token_id = llm_model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T18:14:00.936766Z",
     "iopub.status.busy": "2025-06-22T18:14:00.935924Z",
     "iopub.status.idle": "2025-06-22T18:14:18.286811Z",
     "shell.execute_reply": "2025-06-22T18:14:18.285975Z",
     "shell.execute_reply.started": "2025-06-22T18:14:00.936737Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc0a1e92eae4d0eb6265a362b26be92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample reranker_pair[0]: Query='What are the main arguments for and against Brexit...', Text='EU referendum: why Brexit is a good idea. For the ...'\n",
      "Reranking 30 candidate pairs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6368b98e9cc64b2eb723a33ff7c09565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Type of rerank_scores: <class 'numpy.ndarray'>\n",
      "DEBUG: Shape of rerank_scores: (30,)\n",
      "DEBUG: First 5 rerank_scores: [ 4.54578    5.7194633 -2.2526562 -1.3843509 -3.2141771]\n",
      "\n",
      "--- RAG Pipeline Test ---\n",
      "Query: What are the main arguments for and against Brexit?\n",
      "Top 3 contexts passed to LLM:\n",
      "  Context 1 (Rerank Score: 7.975): and national security a large part of the argument for remaining in. The strongest argument for Brexit is that it is the only way to restore sovereignty to Parliament and escape the jurisdiction of th...\n",
      "  Context 2 (Rerank Score: 6.857): the government in protest over May’s handling of negotiations with the European Union. Here’s what’s going on: In June 2016 the UK held a referendum on whether to stay in the EU or cut ties with the p...\n",
      "  Context 3 (Rerank Score: 5.719): of 2012, according to the Office for National Statistics. Now, economists say the Brexit vote will likely put the brakes on what little forward momentum was left for British economic growth. The cloud...\n",
      "\n",
      "LLM Answer:\n",
      "The main arguments for Brexit are:\n",
      "\n",
      "1. Restoring sovereignty to Parliament and escaping the jurisdiction of the European Court of Justice.\n",
      "2. The UK could no longer control its borders.\n",
      "3. The UK put more money into the EU than it received.\n",
      "4. A cheaper pound would make British exports more competitive in European markets and around the world, helping to narrow a widening trade deficit with Europe.\n",
      "\n",
      "The main arguments against Brexit are:\n",
      "\n",
      "1. The strongest argument for Brexit is that it is the only way to restore sovereignty to Parliament and escape the jurisdiction of the European Court of Justice.\n",
      "2. Mr Cameron’s plan to counter this with an act that reasserts parliamentary sovereignty will not convince many, for the ECJ would still stand supreme.\n",
      "3. In a world with a network of international treaties and obligations, sovereignty is not a completely binary matter.\n",
      "4. Economists say the Brexit vote will likely put the brakes on what little forward momentum was left for British economic growth\n"
     ]
    }
   ],
   "source": [
    "def format_rag_prompt(query: str, context_chunks: list, llm_tokenizer_for_chat=None):\n",
    "    context_str = \"\\n\\n---\\n\\n\".join([chunk['text'] for chunk in context_chunks])\n",
    "\n",
    "    # Truncate context if too long for the LLM (crude truncation)\n",
    "    # A more sophisticated approach would tokenize and count, or summarize\n",
    "    # max_context_len_chars = 7000 # Rough estimate, depends on LLM context window & tokenization\n",
    "    # if len(context_str) > max_context_len_chars:\n",
    "    #     context_str = context_str[:max_context_len_chars] + \"...\"\n",
    "\n",
    "\n",
    "    # Using a chat format is generally better for instruct/chat models\n",
    "    system_message = \"You are a helpful AI assistant. Answer the user's QUESTION based *only* on the provided CONTEXT. If the context does not contain the answer, say 'I cannot answer the question based on the provided context.' Do not use any prior knowledge. Be concise and directly answer the question.\"\n",
    "    \n",
    "    user_message_content = f\"CONTEXT:\\n{context_str}\\n\\nQUESTION: {query}\"\n",
    "\n",
    "    if llm_tokenizer_for_chat: # For Hugging Face Transformers\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message_content}\n",
    "        ]\n",
    "        try:\n",
    "            prompt = llm_tokenizer_for_chat.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            return prompt, system_message, user_message_content # Return components for Ollama\n",
    "        except Exception as e:\n",
    "            print(f\"Error applying chat template: {e}. Falling back to basic prompt.\")\n",
    "            # Fallback basic prompt if chat template fails (less ideal)\n",
    "            prompt = f\"System: {system_message}\\nUser: {user_message_content}\\nAssistant:\"\n",
    "            return prompt, system_message, user_message_content\n",
    "    else: # For manual construction (e.g. if directly using Ollama's non-chat or if tokenizer has no template)\n",
    "         # This part is more for Ollama now, where we send system and user prompts separately\n",
    "        return None, system_message, user_message_content\n",
    "\n",
    "\n",
    "def generate_llm_answer(query: str, context_chunks: list):\n",
    "    if not context_chunks:\n",
    "        return \"No relevant context found to answer the question.\", []\n",
    "\n",
    "    # Option A: Using Hugging Face Transformers LLM\n",
    "    if 'llm_model' in globals() and llm_model is not None and 'llm_tokenizer' in globals() and llm_tokenizer is not None:\n",
    "        formatted_prompt, _, _ = format_rag_prompt(query, context_chunks, llm_tokenizer_for_chat=llm_tokenizer)\n",
    "        \n",
    "        inputs = llm_tokenizer(formatted_prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=4000).to(DEVICE)\n",
    "        \n",
    "        generation_args = {\n",
    "            \"max_new_tokens\": 250,\n",
    "            \"temperature\": 0.1,\n",
    "            \"do_sample\": True,\n",
    "            \"top_p\": 0.9,\n",
    "            \"eos_token_id\": llm_tokenizer.eos_token_id,\n",
    "            \"pad_token_id\": llm_tokenizer.pad_token_id if llm_tokenizer.pad_token_id is not None else llm_tokenizer.eos_token_id\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output_ids = llm_model.generate(**inputs, **generation_args)\n",
    "        \n",
    "        answer = llm_tokenizer.decode(output_ids[0, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "        \n",
    "        # --- THIS IS THE CORRECTED RETURN STATEMENT ---\n",
    "        return answer.strip(), [chunk['text'] for chunk in context_chunks]\n",
    "\n",
    "    # Option B: Using Ollama Chat\n",
    "    elif 'OLLAMA_CHAT_MODEL_NAME' in globals():\n",
    "        _, system_prompt, user_prompt_content = format_rag_prompt(query, context_chunks)\n",
    "        answer = query_ollama_chat(system_prompt, user_prompt_content, model_name=OLLAMA_CHAT_MODEL_NAME)\n",
    "        \n",
    "        # --- THIS IS THE CORRECTED RETURN STATEMENT FOR OLLAMA ---\n",
    "        return answer.strip(), [chunk['text'] for chunk in context_chunks]\n",
    "    \n",
    "    else:\n",
    "        return \"LLM not available or configured.\", []\n",
    "\n",
    "# Test RAG pipeline\n",
    "rag_query = \"What are the main arguments for and against Brexit?\"\n",
    "# Get context from hybrid search\n",
    "retrieved_context_chunks = hybrid_search_and_rerank(rag_query, bm25_k=15, faiss_k=15, rerank_top_n=3)\n",
    "\n",
    "print(f\"\\n--- RAG Pipeline Test ---\")\n",
    "print(f\"Query: {rag_query}\")\n",
    "if retrieved_context_chunks:\n",
    "    print(f\"Top {len(retrieved_context_chunks)} contexts passed to LLM:\")\n",
    "    for i, chunk_info in enumerate(retrieved_context_chunks):\n",
    "        print(f\"  Context {i+1} (Rerank Score: {chunk_info['rerank_score']:.3f}): {chunk_info['text'][:200]}...\")\n",
    "    \n",
    "    # --- THIS IS THE CORRECTED LINE ---\n",
    "    llm_answer, used_context_texts = generate_llm_answer(rag_query, retrieved_context_chunks)\n",
    "    \n",
    "    print(f\"\\nLLM Answer:\\n{llm_answer}\")\n",
    "else:\n",
    "    print(\"No context retrieved for the LLM.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T16:57:30.696042Z",
     "iopub.status.busy": "2025-06-22T16:57:30.695209Z",
     "iopub.status.idle": "2025-06-22T16:57:41.785049Z",
     "shell.execute_reply": "2025-06-22T16:57:41.784311Z",
     "shell.execute_reply.started": "2025-06-22T16:57:30.696017Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving artifacts to: /kaggle/working/rag_artifacts\n",
      "✅ Saved final_df.parquet\n",
      "✅ Saved chunks_df.parquet\n",
      "✅ Saved bm25_index.pkl\n",
      "✅ Saved chunk_embeddings.npy\n",
      "✅ Saved news_chunks.faiss_index\n",
      "\n",
      "All artifacts saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# --- Create a directory to save everything ---\n",
    "ARTIFACTS_DIR = Path(\"/kaggle/working/rag_artifacts\")\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Saving artifacts to: {ARTIFACTS_DIR}\")\n",
    "\n",
    "# 1. Save the final sampled DataFrame\n",
    "df.to_parquet(ARTIFACTS_DIR / \"final_df.parquet\")\n",
    "print(\"✅ Saved final_df.parquet\")\n",
    "\n",
    "# 2. Save the chunks DataFrame\n",
    "chunks_df.to_parquet(ARTIFACTS_DIR / \"chunks_df.parquet\")\n",
    "print(\"✅ Saved chunks_df.parquet\")\n",
    "\n",
    "# 3. Save the BM25 index object using pickle\n",
    "with open(ARTIFACTS_DIR / \"bm25_index.pkl\", \"wb\") as f:\n",
    "    pickle.dump(bm25, f)\n",
    "print(\"✅ Saved bm25_index.pkl\")\n",
    "\n",
    "\n",
    "if 'chunk_embeddings_cpu' in locals():\n",
    "    np.save(ARTIFACTS_DIR / \"chunk_embeddings.npy\", chunk_embeddings_cpu)\n",
    "    print(\"✅ Saved chunk_embeddings.npy\")\n",
    "else:\n",
    "    print(\"⚠️ 'chunk_embeddings_cpu' not found in memory, skipping save. Re-run embedding cell if needed.\")\n",
    "\n",
    "\n",
    "# 5. Save the FAISS index\n",
    "faiss.write_index(index_faiss, str(ARTIFACTS_DIR / \"news_chunks.faiss_index\"))\n",
    "print(\"✅ Saved news_chunks.faiss_index\")\n",
    "\n",
    "print(\"\\nAll artifacts saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T17:03:51.408329Z",
     "iopub.status.busy": "2025-06-22T17:03:51.407612Z",
     "iopub.status.idle": "2025-06-22T17:03:58.578563Z",
     "shell.execute_reply": "2025-06-22T17:03:58.577736Z",
     "shell.execute_reply.started": "2025-06-22T17:03:51.408307Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found artifacts directory. Loading pre-computed files from: /kaggle/working/rag_artifacts\n",
      "Loaded 30000 documents and 278947 chunks.\n",
      "Loaded BM25 index.\n",
      "Loaded FAISS index with 278947 vectors.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "ARTIFACTS_DIR = Path(\"/kaggle/working/rag_artifacts\") \n",
    "# --- Check if artifacts exist and load them ---\n",
    "if ARTIFACTS_DIR.exists() and (ARTIFACTS_DIR / \"final_df.parquet\").exists():\n",
    "    print(f\"✅ Found artifacts directory. Loading pre-computed files from: {ARTIFACTS_DIR}\")\n",
    "    \n",
    "    # 1. Load DataFrames\n",
    "    df = pd.read_parquet(ARTIFACTS_DIR / \"final_df.parquet\")\n",
    "    chunks_df = pd.read_parquet(ARTIFACTS_DIR / \"chunks_df.parquet\")\n",
    "    print(f\"Loaded {len(df)} documents and {len(chunks_df)} chunks.\")\n",
    "\n",
    "    # 2. Load BM25 index\n",
    "    with open(ARTIFACTS_DIR / \"bm25_index.pkl\", \"rb\") as f:\n",
    "        bm25 = pickle.load(f)\n",
    "    print(\"Loaded BM25 index.\")\n",
    "\n",
    "    # 3. Load FAISS index\n",
    "    index_faiss = faiss.read_index(str(ARTIFACTS_DIR / \"news_chunks.faiss_index\"))\n",
    "    print(f\"Loaded FAISS index with {index_faiss.ntotal} vectors.\")\n",
    "    \n",
    "    # chunk_embeddings_cpu = np.load(ARTIFACTS_DIR / \"chunk_embeddings.npy\")\n",
    "    # print(\"Loaded chunk embeddings.\")\n",
    "\n",
    "else:\n",
    "    print(f\"❌ Artifacts not found at {ARTIFACTS_DIR}. Running full data processing pipeline...\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T17:11:44.779361Z",
     "iopub.status.busy": "2025-06-22T17:11:44.778637Z",
     "iopub.status.idle": "2025-06-22T17:12:52.229051Z",
     "shell.execute_reply": "2025-06-22T17:12:52.228266Z",
     "shell.execute_reply.started": "2025-06-22T17:11:44.779336Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping the directory: /kaggle/working/rag_artifacts\n",
      "  adding: kaggle/working/rag_artifacts/ (stored 0%)\n",
      "  adding: kaggle/working/rag_artifacts/news_chunks.faiss_index (deflated 7%)\n",
      "  adding: kaggle/working/rag_artifacts/bm25_index.pkl (deflated 69%)\n",
      "  adding: kaggle/working/rag_artifacts/chunk_embeddings.npy (deflated 7%)\n",
      "  adding: kaggle/working/rag_artifacts/final_df.parquet (deflated 12%)\n",
      "  adding: kaggle/working/rag_artifacts/chunks_df.parquet (deflated 15%)\n",
      "Successfully created zip file at: /kaggle/working/rag_artifacts.zip\n",
      "You can now find 'rag_artifacts.zip' in the Output section of the Data sidebar and download it.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "artifacts_dir = \"/kaggle/working/rag_artifacts\"\n",
    "\n",
    "\n",
    "output_zip_file = \"/kaggle/working/rag_artifacts.zip\"\n",
    "\n",
    "print(f\"Zipping the directory: {artifacts_dir}\")\n",
    "\n",
    "\n",
    "os.system(f\"zip -r {output_zip_file} {artifacts_dir}\")\n",
    "\n",
    "print(f\"Successfully created zip file at: {output_zip_file}\")\n",
    "print(\"You can now find 'rag_artifacts.zip' in the Output section of the Data sidebar and download it.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1705886,
     "sourceId": 2793257,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31042,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
